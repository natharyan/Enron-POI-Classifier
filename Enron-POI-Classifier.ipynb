{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Features to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Features:\n",
      "['poi', 'salary', 'deferral_payments', 'total_payments', 'loan_advances', 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', 'restricted_stock', 'director_fees', 'to_messages', 'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', 'shared_receipt_with_poi']\n",
      "\n",
      "Features Used: 20\n"
     ]
    }
   ],
   "source": [
    "features_list = ['poi','salary', 'deferral_payments', 'total_payments', 'loan_advances', \n",
    "                 'bonus', 'restricted_stock_deferred', 'deferred_income', 'total_stock_value', \n",
    "                 'expenses', 'exercised_stock_options', 'other', 'long_term_incentive', \n",
    "                 'restricted_stock', 'director_fees', 'to_messages', \n",
    "                'from_poi_to_this_person', 'from_messages', 'from_this_person_to_poi', \n",
    "                'shared_receipt_with_poi']\n",
    "\n",
    "print('\\nFeatures:',features_list,sep='\\n')\n",
    "print('\\nFeatures Used:',len(features_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dictionary containing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final_project_dataset.pkl\", \"rb\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'salary': 26704229,\n",
       " 'to_messages': 'NaN',\n",
       " 'deferral_payments': 32083396,\n",
       " 'total_payments': 309886585,\n",
       " 'loan_advances': 83925000,\n",
       " 'bonus': 97343619,\n",
       " 'email_address': 'NaN',\n",
       " 'restricted_stock_deferred': -7576788,\n",
       " 'deferred_income': -27992891,\n",
       " 'total_stock_value': 434509511,\n",
       " 'expenses': 5235198,\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'exercised_stock_options': 311764000,\n",
       " 'from_messages': 'NaN',\n",
       " 'other': 42667589,\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'poi': False,\n",
       " 'long_term_incentive': 48521928,\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'restricted_stock': 130322299,\n",
       " 'director_fees': 1398517}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I first used the following code to find the outliers, these features are known for most people (refer: attached enroninsiderpay pdf)\n",
    "\"\"\"\n",
    "max_outlier = max(data_dict, key= lambda k: data_dict[k]['salary'] + data_dict[k]['bonus'] + data_dict[k]['total_payments'] + data_dict[k]['total_stock_value'] if (data_dict[k]['salary']!='NaN' and data_dict[k]['bonus']!='NaN' and data_dict[k]['total_payments']!='NaN' and data_dict[k]['total_stock_value']!='NaN') else 0)\n",
    "print('Outlier:',max_outlier)\n",
    "print('Salary:', data_dict[max_outlier]['salary'])\n",
    "print('Bonus:', data_dict[max_outlier]['bonus'])\n",
    "data_dict.pop(max_outlier)\n",
    "\"\"\"\n",
    "\n",
    "# In the enron insider pay pdf, there is only one extreme outlier, hence the following is also sufficient\n",
    "data_dict.pop('TOTAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "missing features per person:\n",
      "METTS MARK: 7 poi: False \n",
      "\n",
      "BAXTER JOHN C: 8 poi: False \n",
      "\n",
      "ELLIOTT STEVEN: 10 poi: False \n",
      "\n",
      "CORDES WILLIAM R: 11 poi: False \n",
      "\n",
      "HANNON KEVIN P: 4 poi: True \n",
      "\n",
      "MORDAUNT KRISTINA M: 12 poi: False \n",
      "\n",
      "MEYER ROCKFORD G: 9 poi: False \n",
      "\n",
      "MCMAHON JEFFREY: 5 poi: False \n",
      "\n",
      "HAEDICKE MARK E: 2 poi: False \n",
      "\n",
      "PIPER GREGORY F: 3 poi: False \n",
      "\n",
      "HUMPHREY GENE E: 8 poi: False \n",
      "\n",
      "NOLES JAMES L: 14 poi: False \n",
      "\n",
      "BLACHMAN JEREMY M: 5 poi: False \n",
      "\n",
      "SUNDE MARTIN: 7 poi: False \n",
      "\n",
      "GIBBS DANA R: 9 poi: False \n",
      "\n",
      "LOWRY CHARLES P: 15 poi: False \n",
      "\n",
      "COLWELL WESLEY: 5 poi: True \n",
      "\n",
      "MULLER MARK S: 4 poi: False \n",
      "\n",
      "JACKSON CHARLENE R: 6 poi: False \n",
      "\n",
      "WESTFAHL RICHARD K: 11 poi: False \n",
      "\n",
      "WALTERS GARETH W: 14 poi: False \n",
      "\n",
      "WALLS JR ROBERT H: 5 poi: False \n",
      "\n",
      "KITCHEN LOUISE: 6 poi: False \n",
      "\n",
      "CHAN RONNIE: 15 poi: False \n",
      "\n",
      "BELFER ROBERT: 13 poi: False \n",
      "\n",
      "SHANKMAN JEFFREY A: 5 poi: False \n",
      "\n",
      "WODRASKA JOHN: 17 poi: False \n",
      "\n",
      "BERGSIEKER RICHARD P: 5 poi: False \n",
      "\n",
      "URQUHART JOHN A: 15 poi: False \n",
      "\n",
      "BIBI PHILIPPE A: 5 poi: False \n",
      "\n",
      "RIEKER PAULA H: 4 poi: True \n",
      "\n",
      "WHALEY DAVID A: 17 poi: False \n",
      "\n",
      "BECK SALLY W: 7 poi: False \n",
      "\n",
      "HAUG DAVID L: 10 poi: False \n",
      "\n",
      "ECHOLS JOHN B: 10 poi: False \n",
      "\n",
      "MENDELSOHN JOHN: 15 poi: False \n",
      "\n",
      "HICKERSON GARY J: 6 poi: False \n",
      "\n",
      "CLINE KENNETH W: 16 poi: False \n",
      "\n",
      "LEWIS RICHARD: 12 poi: False \n",
      "\n",
      "HAYES ROBERT E: 10 poi: False \n",
      "\n",
      "KOPPER MICHAEL J: 11 poi: True \n",
      "\n",
      "LEFF DANIEL P: 7 poi: False \n",
      "\n",
      "LAVORATO JOHN J: 5 poi: False \n",
      "\n",
      "BERBERIAN DAVID: 13 poi: False \n",
      "\n",
      "DETMERING TIMOTHY J: 7 poi: False \n",
      "\n",
      "WAKEHAM JOHN: 16 poi: False \n",
      "\n",
      "POWERS WILLIAM: 12 poi: False \n",
      "\n",
      "GOLD JOSEPH: 11 poi: False \n",
      "\n",
      "BANNANTINE JAMES M: 5 poi: False \n",
      "\n",
      "DUNCAN JOHN H: 14 poi: False \n",
      "\n",
      "SHAPIRO RICHARD S: 6 poi: False \n",
      "\n",
      "SHERRIFF JOHN R: 6 poi: False \n",
      "\n",
      "SHELBY REX: 5 poi: True \n",
      "\n",
      "LEMAISTRE CHARLES: 14 poi: False \n",
      "\n",
      "DEFFNER JOSEPH M: 5 poi: False \n",
      "\n",
      "KISHKILL JOSEPH G: 12 poi: False \n",
      "\n",
      "WHALLEY LAWRENCE G: 5 poi: False \n",
      "\n",
      "MCCONNELL MICHAEL S: 5 poi: False \n",
      "\n",
      "PIRO JIM: 12 poi: False \n",
      "\n",
      "DELAINEY DAVID W: 5 poi: True \n",
      "\n",
      "SULLIVAN-SHAKLOVITZ COLLEEN: 11 poi: False \n",
      "\n",
      "WROBEL BRUCE: 17 poi: False \n",
      "\n",
      "LINDHOLM TOD A: 9 poi: False \n",
      "\n",
      "MEYER JEROME J: 15 poi: False \n",
      "\n",
      "LAY KENNETH L: 2 poi: True \n",
      "\n",
      "BUTTS ROBERT H: 10 poi: False \n",
      "\n",
      "OLSON CINDY K: 4 poi: False \n",
      "\n",
      "MCDONALD REBECCA: 11 poi: False \n",
      "\n",
      "CUMBERLAND MICHAEL S: 11 poi: False \n",
      "\n",
      "GAHN ROBERT S: 9 poi: False \n",
      "\n",
      "BADUM JAMES P: 14 poi: False \n",
      "\n",
      "HERMANN ROBERT J: 9 poi: False \n",
      "\n",
      "FALLON JAMES B: 5 poi: False \n",
      "\n",
      "GATHMANN WILLIAM D: 15 poi: False \n",
      "\n",
      "HORTON STANLEY C: 9 poi: False \n",
      "\n",
      "BOWEN JR RAYMOND M: 5 poi: True \n",
      "\n",
      "GILLIS JOHN: 16 poi: False \n",
      "\n",
      "FITZGERALD JAY L: 5 poi: False \n",
      "\n",
      "MORAN MICHAEL P: 11 poi: False \n",
      "\n",
      "REDMOND BRIAN L: 8 poi: False \n",
      "\n",
      "BAZELIDES PHILIP J: 12 poi: False \n",
      "\n",
      "BELDEN TIMOTHY N: 4 poi: True \n",
      "\n",
      "DIMICHELE RICHARD G: 10 poi: False \n",
      "\n",
      "DURAN WILLIAM D: 5 poi: False \n",
      "\n",
      "THORN TERENCE H: 5 poi: False \n",
      "\n",
      "FASTOW ANDREW S: 10 poi: True \n",
      "\n",
      "FOY JOE: 10 poi: False \n",
      "\n",
      "CALGER CHRISTOPHER F: 5 poi: True \n",
      "\n",
      "RICE KENNETH D: 4 poi: True \n",
      "\n",
      "KAMINSKI WINCENTY J: 5 poi: False \n",
      "\n",
      "LOCKHART EUGENE E: 19 poi: False \n",
      "\n",
      "COX DAVID: 5 poi: False \n",
      "\n",
      "OVERDYKE JR JERE C: 11 poi: False \n",
      "\n",
      "PEREIRA PAULO V. FERRAZ: 15 poi: False \n",
      "\n",
      "STABLER FRANK: 12 poi: False \n",
      "\n",
      "SKILLING JEFFREY K: 5 poi: True \n",
      "\n",
      "BLAKE JR. NORMAN P: 15 poi: False \n",
      "\n",
      "SHERRICK JEFFREY B: 11 poi: False \n",
      "\n",
      "PRENTICE JAMES: 14 poi: False \n",
      "\n",
      "GRAY RODNEY: 14 poi: False \n",
      "\n",
      "THE TRAVEL AGENCY IN THE PARK: 17 poi: False \n",
      "\n",
      "UMANOFF ADAM S: 10 poi: False \n",
      "\n",
      "KEAN STEVEN J: 5 poi: False \n",
      "\n",
      "FOWLER PEGGY: 11 poi: False \n",
      "\n",
      "WASAFF GEORGE: 4 poi: False \n",
      "\n",
      "WHITE JR THOMAS E: 11 poi: False \n",
      "\n",
      "CHRISTODOULOU DIOMEDES: 16 poi: False \n",
      "\n",
      "ALLEN PHILLIP K: 2 poi: False \n",
      "\n",
      "SHARP VICTORIA T: 4 poi: False \n",
      "\n",
      "JAEDICKE ROBERT: 12 poi: False \n",
      "\n",
      "WINOKUR JR. HERBERT S: 15 poi: False \n",
      "\n",
      "BROWN MICHAEL: 12 poi: False \n",
      "\n",
      "MCCLELLAN GEORGE: 5 poi: False \n",
      "\n",
      "HUGHES JAMES A: 11 poi: False \n",
      "\n",
      "REYNOLDS LAWRENCE: 7 poi: False \n",
      "\n",
      "PICKERING MARK R: 7 poi: False \n",
      "\n",
      "BHATNAGAR SANJAY: 8 poi: False \n",
      "\n",
      "CARTER REBECCA C: 6 poi: False \n",
      "\n",
      "BUCHANAN HAROLD G: 5 poi: False \n",
      "\n",
      "YEAP SOON: 15 poi: False \n",
      "\n",
      "MURRAY JULIA H: 5 poi: False \n",
      "\n",
      "GARLAND C KEVIN: 5 poi: False \n",
      "\n",
      "DODSON KEITH: 9 poi: False \n",
      "\n",
      "YEAGER F SCOTT: 12 poi: True \n",
      "\n",
      "HIRKO JOSEPH: 13 poi: True \n",
      "\n",
      "DIETRICH JANET R: 5 poi: False \n",
      "\n",
      "DERRICK JR. JAMES V: 3 poi: False \n",
      "\n",
      "FREVERT MARK A: 2 poi: False \n",
      "\n",
      "PAI LOU L: 11 poi: False \n",
      "\n",
      "HAYSLETT RODERICK J: 12 poi: False \n",
      "\n",
      "BAY FRANKLIN R: 9 poi: False \n",
      "\n",
      "MCCARTY DANNY J: 11 poi: False \n",
      "\n",
      "FUGH JOHN L: 15 poi: False \n",
      "\n",
      "SCRIMSHAW MATTHEW: 17 poi: False \n",
      "\n",
      "KOENIG MARK E: 5 poi: True \n",
      "\n",
      "SAVAGE FRANK: 16 poi: False \n",
      "\n",
      "IZZO LAWRENCE L: 6 poi: False \n",
      "\n",
      "TILNEY ELIZABETH A: 5 poi: False \n",
      "\n",
      "MARTIN AMANDA K: 6 poi: False \n",
      "\n",
      "BUY RICHARD B: 4 poi: False \n",
      "\n",
      "GRAMM WENDY L: 17 poi: False \n",
      "\n",
      "CAUSEY RICHARD A: 5 poi: True \n",
      "\n",
      "TAYLOR MITCHELL S: 7 poi: False \n",
      "\n",
      "DONAHUE JR JEFFREY M: 5 poi: False \n",
      "\n",
      "GLISAN JR BEN F: 5 poi: True \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# total NaN values fo reach poi\n",
    "check_poi_NaN = {}\n",
    "for feature in features_list:\n",
    "    check_poi_NaN[feature] = 0\n",
    "\n",
    "# total values for each feature as tuples (poi, non-poi)\n",
    "features_values = {}\n",
    "for feature in features_list:\n",
    "    features_values[feature] = [0,0]\n",
    "print(\"\\nmissing features per person:\")\n",
    "# number of hidden values per person and the most hidden value among all known poi's\n",
    "for person in data_dict:\n",
    "    count = 0\n",
    "    for feature in features_list:\n",
    "        if data_dict[person][feature] == 'NaN':\n",
    "            # print(feature, end=', ')\n",
    "            count+=1\n",
    "            if data_dict[person]['poi']:\n",
    "                check_poi_NaN[feature] += 1\n",
    "        else:\n",
    "            if data_dict[person]['poi']:\n",
    "                features_values[feature][0]+=data_dict[person][feature]\n",
    "            else:\n",
    "                features_values[feature][1]+=data_dict[person][feature]\n",
    "    print(f'{person}:',count,'poi:',data_dict[person]['poi'],'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"number of people in the dataset:\",len(data_dict))\n",
    "print()\n",
    "print(\"number of culpable people (known poi's):\",len([person for person in data_dict if data_dict[person]['poi']]))\n",
    "print(\"number of features used:\",len(features_list),'\\n')\n",
    "print(\"features with missing values:\",[key for key in check_poi_NaN if check_poi_NaN[key]!=0],'\\n')\n",
    "print(\"most poi's are hiding their:\",max(check_poi_NaN,key= lambda k: check_poi_NaN[k]))\n",
    "print()\n",
    "total_feature_values = list(features_values.values())\n",
    "\n",
    "# taking absolute value for features based on negative defered values \n",
    "feature_differences = [abs(i[0])/len([person for person in data_dict if data_dict[person]['poi']]) - abs(i[1])/(len(data_dict) - len([person for person in data_dict if data_dict[person]['poi']])) for i in total_feature_values ]\n",
    "max_dif_features = []\n",
    "\n",
    "for i in sorted(feature_differences,reverse=True): max_dif_features+=[features_list[feature_differences.index(i)]]\n",
    "\n",
    "print(\"features with maximum differences between poi's and non-poi's:\")\n",
    "print(max_dif_features[2:5])\n",
    "print(sorted(feature_differences,reverse=True)[2:5])\n",
    "# I observed that on average the exercised stock options of poi's are much larger than that of non-poi's\n",
    "\n",
    "# plotting this feature against their salary\n",
    "x_values_poi = [data_dict[person]['salary'] for person in data_dict if data_dict[person]['poi'] and data_dict[person]['salary']!='NaN' and data_dict[person]['exercised_stock_options']!='NaN']\n",
    "y_values_poi = [data_dict[person]['exercised_stock_options'] for person in data_dict if data_dict[person]['poi'] and data_dict[person]['salary']!='NaN' and data_dict[person]['exercised_stock_options']!='NaN']\n",
    "x_values_not_poi = [data_dict[person]['salary'] for person in data_dict if not data_dict[person]['poi'] and data_dict[person]['salary']!='NaN' and data_dict[person]['exercised_stock_options']!='NaN']\n",
    "y_values_not_poi = [data_dict[person]['exercised_stock_options'] for person in data_dict if not data_dict[person]['poi'] and data_dict[person]['salary']!='NaN' and data_dict[person]['exercised_stock_options']!='NaN']\n",
    "plt.scatter(x_values_poi, y_values_poi, color='r', marker='o', label='poi')\n",
    "plt.scatter(x_values_not_poi, y_values_not_poi, color='b', marker='o', label='not poi')\n",
    "plt.xlabel('Salary')\n",
    "plt.ylabel('Exercised Stock Options')\n",
    "plt.title('Salary vs. Exercised Stock Options')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "# I'm creating two new features. One stores the proportion of messages sent to poi's and the other stores the proportion of messages received from poi's\n",
    "for person in my_dataset:\n",
    "    if my_dataset[person]['from_this_person_to_poi']!='NaN' and my_dataset[person]['from_messages']!='NaN':\n",
    "        my_dataset[person]['ratio_to_poi'] = my_dataset[person]['from_this_person_to_poi']/my_dataset[person]['from_messages']\n",
    "    else:\n",
    "        my_dataset[person]['ratio_to_poi'] = 0\n",
    "    if my_dataset[person]['from_poi_to_this_person']!='NaN' and my_dataset[person]['to_messages']!='NaN':\n",
    "        my_dataset[person]['ratio_from_poi'] = my_dataset[person]['from_poi_to_this_person']/my_dataset[person]['to_messages']\n",
    "    else:\n",
    "        my_dataset[person]['ratio_from_poi'] = 0\n",
    "features_list.extend(['ratio_from_poi','ratio_to_poi'])\n",
    "\n",
    "\n",
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "selected features: ('exercised_stock_options', 'total_stock_value', 'bonus', 'salary', 'ratio_to_poi', 'deferred_income', 'long_term_incentive', 'restricted_stock') \n",
      "\n",
      "scores: (25.09754152873549, 24.4676540475264, 21.06000170753657, 18.575703268041785, 16.64170707046899, 11.5955476597306, 10.072454529369441, 9.346700791051488) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selecting 8 best features\n",
    "kn = 9\n",
    "k_best = SelectKBest(k=kn)\n",
    "k_best.fit(features,labels)\n",
    "\n",
    "feature_score_array = [(features_list[i+1],k_best.scores_[i]) for i in range(len(k_best.scores_))]\n",
    "feature_score_array = sorted(feature_score_array, key=lambda x: x[1],reverse=True)\n",
    "\n",
    "k_best_features,k_best_features_scores = zip(*feature_score_array)\n",
    "k_best_features,k_best_features_scores = k_best_features[:kn-1],k_best_features_scores[:kn-1]\n",
    "\n",
    "print()\n",
    "print(\"selected features:\",k_best_features,'\\n')\n",
    "print(\"scores:\",k_best_features_scores,'\\n')\n",
    "\n",
    "# recreating features and labels with new selected features\n",
    "data = featureFormat(my_dataset, ('poi',)+k_best_features, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying a Variety of Classfiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.94      0.93        36\n",
      "         1.0       0.60      0.50      0.55         6\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.76      0.72      0.74        42\n",
      "weighted avg       0.87      0.88      0.88        42\n",
      "\n",
      "Support Vector Classification\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      1.00      0.92        36\n",
      "         1.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.43      0.50      0.46        42\n",
      "weighted avg       0.73      0.86      0.79        42\n",
      "\n",
      "Decision Tree Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.89      0.91        36\n",
      "         1.0       0.50      0.67      0.57         6\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.72      0.78      0.74        42\n",
      "weighted avg       0.88      0.86      0.87        42\n",
      "\n",
      "Random Forest Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      1.00      0.95        36\n",
      "         1.0       1.00      0.33      0.50         6\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.95      0.67      0.72        42\n",
      "weighted avg       0.91      0.90      0.88        42\n",
      "\n",
      "Classifier Implementing the K-Nearest Neighbors Vote:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      1.00      0.95        36\n",
      "         1.0       1.00      0.33      0.50         6\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.95      0.67      0.72        42\n",
      "weighted avg       0.91      0.90      0.88        42\n",
      "\n",
      "AdaBoost Classifier:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.94      0.93        36\n",
      "         1.0       0.60      0.50      0.55         6\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.76      0.72      0.74        42\n",
      "weighted avg       0.87      0.88      0.88        42\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aryannath/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aryannath/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/aryannath/miniconda3/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "\n",
    "\n",
    "clf.fit(features_train,labels_train)\n",
    "print(\"Gaussian Naive Bayes\")\n",
    "pred = clf.predict(features_test)\n",
    "print(classification_report(labels_test,pred))\n",
    "\n",
    "# performance without parameter tuning:\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.94      0.93        36\n",
    "#          1.0       0.60      0.50      0.55         6\n",
    "\n",
    "#     accuracy                           0.88        42\n",
    "#    macro avg       0.76      0.72      0.74        42\n",
    "# weighted avg       0.87      0.88      0.88        42\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(random_state=0)\n",
    "clf.fit(features_train,labels_train)\n",
    "print(\"Support Vector Classification\")\n",
    "pred = clf.predict(features_test)\n",
    "print(classification_report(labels_test,pred))\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier(random_state=0,max_depth=2)\n",
    "clf.fit(features_train,labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print(\"Decision Tree Classifier\")\n",
    "print(classification_report(labels_test,pred))\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.94      0.89      0.91        36\n",
    "#          1.0       0.50      0.67      0.57         6\n",
    "\n",
    "#     accuracy                           0.86        42\n",
    "#    macro avg       0.72      0.78      0.74        42\n",
    "# weighted avg       0.88      0.86      0.87        42\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0,max_depth=2)\n",
    "clf.fit(features_train,labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print(\"Random Forest Classifier\")\n",
    "print(classification_report(labels_test,pred))\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.90      1.00      0.95        36\n",
    "#          1.0       1.00      0.33      0.50         6\n",
    "\n",
    "#     accuracy                           0.90        42\n",
    "#    macro avg       0.95      0.67      0.72        42\n",
    "# weighted avg       0.91      0.90      0.88        42\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(features_train,labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print(\"Classifier Implementing the K-Nearest Neighbors Vote:\")\n",
    "print(classification_report(labels_test,pred))\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.90      1.00      0.95        36\n",
    "#          1.0       1.00      0.33      0.50         6\n",
    "\n",
    "#     accuracy                           0.90        42\n",
    "#    macro avg       0.95      0.67      0.72        42\n",
    "# weighted avg       0.91      0.90      0.88        42\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=40, random_state=0)\n",
    "clf.fit(features_train,labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print(\"AdaBoost Classifier:\")\n",
    "print(classification_report(labels_test,pred))\n",
    "\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#          0.0       0.92      0.94      0.93        36\n",
    "#          1.0       0.60      0.50      0.55         6\n",
    "\n",
    "#     accuracy                           0.88        42\n",
    "#    macro avg       0.76      0.72      0.74        42\n",
    "# weighted avg       0.87      0.88      0.88        42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter Tuned Classifiers:\n",
      "\n",
      "Decision Tree Classifer:\n",
      "\n",
      "best parameters:\n",
      "{'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'random_state': 0, 'splitter': 'random'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.83      0.88        36\n",
      "         1.0       0.40      0.67      0.50         6\n",
      "\n",
      "    accuracy                           0.81        42\n",
      "   macro avg       0.67      0.75      0.69        42\n",
      "weighted avg       0.86      0.81      0.83        42\n",
      "\n",
      "[[30  6]\n",
      " [ 2  4]]\n",
      "\n",
      "AdaBoost Classifier:\n",
      "\n",
      "best parameters:\n",
      "{'algorithm': 'SAMME', 'learning_rate': 1, 'n_estimators': 25}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.92      0.92      0.92        36\n",
      "         1.0       0.50      0.50      0.50         6\n",
      "\n",
      "    accuracy                           0.86        42\n",
      "   macro avg       0.71      0.71      0.71        42\n",
      "weighted avg       0.86      0.86      0.86        42\n",
      "\n",
      "[[33  3]\n",
      " [ 3  3]]\n",
      "\n",
      "Random Forest Classifier:\n",
      "\n",
      "best parameters:\n",
      "{'criterion': 'gini', 'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n_estimators': 10, 'random_state': 0}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      1.00      0.95        36\n",
      "         1.0       1.00      0.33      0.50         6\n",
      "\n",
      "    accuracy                           0.90        42\n",
      "   macro avg       0.95      0.67      0.72        42\n",
      "weighted avg       0.91      0.90      0.88        42\n",
      "\n",
      "[[36  0]\n",
      " [ 4  2]]\n",
      "\n",
      "Classifier Implementing the K-Nearest Neighbors Vote:\n",
      "\n",
      "best parameters:\n",
      "{'algorithm': 'auto', 'n_neighbors': 3, 'p': 1, 'weights': 'uniform'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.88      1.00      0.94        36\n",
      "         1.0       1.00      0.17      0.29         6\n",
      "\n",
      "    accuracy                           0.88        42\n",
      "   macro avg       0.94      0.58      0.61        42\n",
      "weighted avg       0.90      0.88      0.84        42\n",
      "\n",
      "[[36  0]\n",
      " [ 5  1]]\n"
     ]
    }
   ],
   "source": [
    "def bestParams(clf,cv,param_grid,score):\n",
    "    '''\n",
    "    This function takes a classifier, features, target variable, paramter grid (for grid search) \n",
    "    and fits and scores the resulting model.\n",
    "    A confusion matrix and classification report is returned \n",
    "    with the performance metrics.\n",
    "    Takes inspiration from:  http://chrisstrelioff.ws/sandbox/2015/06/25/decision_trees_in_python_again_cross_validation.html\n",
    "    And the know how to tune gridsearch from:\n",
    "    https://www.kaggle.com/kevinarvai/fine-tuning-a-classifier-in-scikit-learn\n",
    "    '''\n",
    "    from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "   \n",
    "    skf = StratifiedKFold(n_splits=cv)\n",
    "    \n",
    "    if param_grid != None:\n",
    "        clf = GridSearchCV(clf, param_grid=param_grid, cv=skf,\n",
    "                           scoring=score,\n",
    "                           return_train_score=True, n_jobs=-1)\n",
    "        clf.fit(features_train, labels_train)\n",
    "     \n",
    "        print()\n",
    "        print(\"best parameters:\")\n",
    "        print(clf.best_params_)\n",
    "        bestP = clf.best_params_\n",
    "    else:\n",
    "        clf.fit(features_train, labels_train)\n",
    "        bestP= []\n",
    "    return bestP\n",
    "\n",
    "print(\"Parameter Tuned Classifiers:\")\n",
    "\n",
    "print(\"\\nDecision Tree Classifer:\")\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "param_grid = [{'criterion': ['gini'],\n",
    "  'splitter': ['best'],\n",
    "  'max_depth': [None, 1, 2, 3, 4, 5],\n",
    "  'min_samples_split': [2, 5, 10],\n",
    "  'min_samples_leaf': [1, 2, 4],\n",
    "  'random_state': [0, 42, 123]},\n",
    " {'criterion': ['entropy'],\n",
    "  'splitter': ['best'],\n",
    "  'max_depth': [None, 1, 2, 3, 4, 5],\n",
    "  'min_samples_split': [2, 5, 10],\n",
    "  'min_samples_leaf': [1, 2, 4],\n",
    "  'random_state': [0, 42, 123]},\n",
    "  {'criterion': ['gini'],\n",
    "  'splitter': ['random'],\n",
    "  'max_depth': [None, 1, 2, 3, 4, 5],\n",
    "  'min_samples_split': [2, 5, 10],\n",
    "  'min_samples_leaf': [1, 2, 4],\n",
    "  'random_state': [0, 42, 123]}]\n",
    "\n",
    "bestParameters = bestParams(clf,10,param_grid,\"recall\")\n",
    "clf = DecisionTreeClassifier(**bestParameters)\n",
    "clf.fit(features_train,labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print(classification_report(labels_test,pred))\n",
    "print(confusion_matrix(labels_test,pred))\n",
    "\n",
    "\n",
    "print(\"\\nAdaBoost Classifier:\")\n",
    "clf = AdaBoostClassifier()\n",
    "\n",
    "param_grid = [{'algorithm':['SAMME'],\n",
    "                'n_estimators':[25,35,40,50,75,100],\n",
    "                'learning_rate':[0.1,0.25,0.5,0.75,1]},\n",
    "                {'algorithm':['SAMME.R'],\n",
    "                'n_estimators':[25,35,40,50,75,100],\n",
    "                'learning_rate':[0.1,0.25,0.5,0.75,1]}]\n",
    "\n",
    "bestParameters = bestParams(clf,10,param_grid,\"recall\")\n",
    "clf = AdaBoostClassifier(**bestParameters)\n",
    "clf.fit(features_train,labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print(classification_report(labels_test,pred))\n",
    "print(confusion_matrix(labels_test,pred))\n",
    "\n",
    "\n",
    "print(\"\\nRandom Forest Classifier:\")\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "param_grid = [{'n_estimators': [10, 50, 100, 200],\n",
    "  'criterion': ['gini', 'entropy'],\n",
    "  'max_depth': [None, 5, 10, 15, 20],\n",
    "  'min_samples_split': [2, 5, 10],\n",
    "  'min_samples_leaf': [1, 2, 4],\n",
    "  'random_state': [0, 42, 123]},\n",
    " {'n_estimators': [10, 50, 100, 200],\n",
    "  'criterion': ['gini', 'entropy'],\n",
    "  'max_depth': [None, 25, 30, 35, 40],\n",
    "  'min_samples_split': [2, 5, 10],\n",
    "  'min_samples_leaf': [1, 2, 4],\n",
    "  'random_state': [0, 42, 123]}]\n",
    "\n",
    "bestParameters = bestParams(clf,10,param_grid,\"recall\")\n",
    "clf = RandomForestClassifier(**bestParameters)\n",
    "clf.fit(features_train,labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print(classification_report(labels_test,pred))\n",
    "print(confusion_matrix(labels_test,pred))\n",
    "\n",
    "\n",
    "print(\"\\nClassifier Implementing the K-Nearest Neighbors Vote:\")\n",
    "clf = KNeighborsClassifier()\n",
    "\n",
    "param_grid = [{'n_neighbors': [3, 5, 7, 9, 11],\n",
    "  'weights': ['uniform', 'distance'],\n",
    "  'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "  'p': [1, 2]},\n",
    " {'n_neighbors': [13, 15, 17, 19, 21],\n",
    "  'weights': ['uniform', 'distance'],\n",
    "  'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "  'p': [1, 2]}]\n",
    "\n",
    "bestParameters = bestParams(clf,10,param_grid,\"recall\")\n",
    "clf = KNeighborsClassifier(**bestParameters)\n",
    "clf.fit(features_train,labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "print(classification_report(labels_test,pred))\n",
    "print(confusion_matrix(labels_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: new feature - counter for occurence of stem words found very commonly in emails between known poi's"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
